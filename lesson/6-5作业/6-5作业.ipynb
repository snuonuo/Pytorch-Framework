{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【任务1】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务名称：**  \n",
    "weight_decay；dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务简介：**  \n",
    "了解正则化中L1和L2（weight decay）；了解dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**详细说明：**  \n",
    "本节第一部分讲解正则化的概念，正则化方法是机器学习（深度学习）中重要的方法，它目的在于减小方差。常用的正则化方法有L1和L2正则化，其中L2正则化又称为weight decay。在pytorch的优化器中就提供了weight decay的实现，本节课将学习weight decay的pytorch实现。\n",
    "\n",
    "本节第二部分讲解深度学习中常见的正则化方法——Dropout，Dropout是简洁高效的正则化方法，但需要注意其在实现过程中的权值数据尺度问题。本节课将详细介绍pytorch中Dropout的实现细节。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业名称（详解）：**  \n",
    "1. weight decay在pytorch的SGD中实现代码是哪一行？它对应的数学公式为？\n",
    "\n",
    "2. PyTorch中，Dropout在训练的时候权值尺度会进行什么操作？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "SGD实现：\n",
    "\n",
    "optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2)\n",
    "\n",
    "查看pytorch API 里的SGD来看这些整组化方法，另外要学会使用断点调试。\n",
    "\n",
    "\n",
    "数学公式为：\n",
    "\n",
    "$w_{i+1} = (1-\\lambda)*w_i - \\frac{\\delta Loss}{\\delta w_i}$ \n",
    "\n",
    "\n",
    "2. dropout的时候权值的尺度会缩小p倍，p是dropout的概率，因此测试的时候也会将网络尺度缩小p倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【任务2】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务名称：**  \n",
    "Batch Normalization；Layer Normalizatoin、Instance Normalizatoin和Group Normalizatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务简介：**  \n",
    "学习深度学习中常见的标准化方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**详细说明：**  \n",
    "本节第一部分介绍深度学习中最重要的一个 Normalizatoin方法——Batch Normalization，并分析其计算方式，同时讲解PyTorch中nn.BatchNorm1d、nn.BatchNorm2d、nn.BatchNorm3d三种BN的计算方式及原理。\n",
    "\n",
    "本节第二部分介绍2015年之后出现的常见的Normalization方法——Layer Normalizatoin、Instance Normalizatoin和Group Normalizatoin，分析各Normalization的由来与应用场景，同时对比分析BN，LN，IN和GN之间的计算差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业名称（详解）：**  \n",
    "1. Batch Normalization 的4个重要参数分别是什么？BN层中采用这个四个参数对X做了什么操作？\n",
    "\n",
    "2. 课程结尾的 “加减乘除”是什么意思？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 分别是：均值，方差，$\\beta$ 和 $\\gamma$\n",
    "\n",
    "$x_i = \\frac{x_i-\\mu}{ \\sqrt{\\sigma^2+\\epsilon} }$\n",
    "\n",
    "$y_i = \\gamma*x_i+\\beta$\n",
    "\n",
    "\n",
    "2. 加$\\beta$, 乘$\\gamma$，减均值，除方差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
