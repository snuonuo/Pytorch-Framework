{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【任务1】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务名称：**  \n",
    "\n",
    "权值初始化；损失函数（一）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务简介：** \n",
    "\n",
    "学习权值初始化的原理；介绍损失函数、代价函数与目标函数的关系，并学习交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**详细说明：**  \n",
    "\n",
    "本节第一部分讲解权值初始化的必要性，首先分析神经网络中权值的方差过大导致梯度爆炸的原因，然后从方差一致性原则出发分析Xavier初始化方法与Kaiming初始化方法的由来，最后介绍pytorch提供的十种初始化方法。\n",
    "\n",
    "本节第二部分介绍损失函数、代价函数与目标函数的联系与不同之处，然后学习人民币二分类任务中使用到的交叉熵损失函数，在讲解交叉熵损失函数时补充分析自信息、信息熵、相对熵和交叉熵之间的关系，最后学习四种损失函数：\n",
    "\n",
    "1. nn.CrossEntropyLoss\n",
    "2. nn.NLLLoss\n",
    "3. nn.BCELoss\n",
    "4. nn.BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业名称（详解）：**  \n",
    "\n",
    "1. Lossfunction依旧属于网络层的概念，即仍旧是Module的子类，为了对lossfunction有一个更清晰的概念，需要大家采用步进(Step into)的调试方法从loss_functoin = nn.CrossEntropyLoss()  语句进入函数，观察从nn.CrossEntropyLoss()到class Module(object)一共经历了哪些类，记录其中所有进入的类及函数。\n",
    "   例如：  \n",
    "   第一步：CrossEntropyLoss类，super(CrossEntropyLoss, self).__init__  \n",
    "   第二步：……  \n",
    "   第三步：……  \n",
    "   第n步：进入Module 类 \n",
    "\n",
    "2. 损失函数的reduction有三种模式，它们的作用分别是什么？\n",
    "\n",
    "   当inputs和target及weight分别如以下参数时，reduction=’mean’模式时，loss是如何计算得到的？\n",
    "   \n",
    "   inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)\n",
    "   \n",
    "   target = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "   \n",
    "   weights = torch.tensor([1, 2], dtype=torch.float）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lossfunction依旧属于网络层的概念，即仍旧是Module的子类，为了对lossfunction有一个更清晰的概念，需要大家采用步进(Step into)的调试方法从loss_functoin = nn.CrossEntropyLoss()  语句进入函数，观察从nn.CrossEntropyLoss()到class Module(object)一共经历了哪些类，记录其中所有进入的类及函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：CrossEntropyLoss类，super(CrossEntropyLoss, self).__init__ \n",
    "\n",
    "第二步：CrossEntropyLoss类，super(_WeightedLoss, self).__init__ \n",
    "\n",
    "第三步：CrossEntropyLoss类，super(_Loss, self).__init__() \n",
    "\n",
    "第四步：进入Module 类，torch._C._log_api_usage_once(\"python.nn_module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 损失函数的reduction有三种模式，它们的作用分别是什么？\n",
    "\n",
    "   当inputs和target及weight分别如以下参数时，reduction=’mean’模式时，loss是如何计算得到的？\n",
    "   \n",
    "   inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)\n",
    "   \n",
    "   target = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "   \n",
    "   weights = torch.tensor([1, 2], dtype=torch.float）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3642)\n",
      "1.5671179294586182\n"
     ]
    }
   ],
   "source": [
    "# reduction=None就是对进行处理\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], dtype=torch.float)\n",
    "target = torch.tensor([0, 1, 1], dtype=torch.long)\n",
    "weights = torch.tensor([1, 2], dtype=torch.float)\n",
    "                       \n",
    "loss_ = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "loss = loss_(inputs, target) \n",
    "print(loss)\n",
    " \n",
    "loss_res = 0    \n",
    "for idx in range(3):\n",
    "    \n",
    "    input_1 = inputs.detach().numpy()[idx]      # [1, 2]\n",
    "    target_1 = target.numpy()[idx]              # [0]\n",
    "\n",
    "    # 第一项\n",
    "    x_class = input_1[target_1]\n",
    "\n",
    "    # 第二项\n",
    "    sigma_exp_x = np.sum(list(map(np.exp, input_1)))\n",
    "    log_sigma_exp_x = np.log(sigma_exp_x)\n",
    "\n",
    "    # 输出loss\n",
    "    loss = -x_class + log_sigma_exp_x\n",
    "    loss_res += loss\n",
    "\n",
    "print(loss_res.mean())\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【任务2】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务名称：**  \n",
    "\n",
    "pytorch的14种损失函数；优化器optimizer的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务简介：**  \n",
    "\n",
    "学习pytorch中剩下的14种损失函数；学习优化器optimizer的基本属性、基本方法和作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**详细说明：**  \n",
    "本节第一部分学习pytorch的另外14种损失函数：\n",
    "\n",
    "5. nn.L1Loss\n",
    "6. nn.MSELoss\n",
    "7. nn.SmoothL1Loss\n",
    "8. nn.PoissonNLLLoss\n",
    "9. nn.KLDivLoss\n",
    "10. nn.MarginRankingLoss\n",
    "11. nn.MultiLabelMarginLoss\n",
    "12. nn.SoftMarginLoss\n",
    "13. nn.MultiLabelSoftMarginLoss\n",
    "14. nn.MultiMarginLoss\n",
    "15. nn.TripletMarginLoss\n",
    "16. nn.HingeEmbeddingLoss\n",
    "17. nn.CosineEmbeddingLoss\n",
    "18. nn.CTCLoss\n",
    "    本节第二部分学习优化器的基本概念，了解pytorch中optimizer的基本属性和方法 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业名称（详解）：**  \n",
    "\n",
    "1. 总结所学习的18种损失函数，制作思维导图或总结表等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【任务3】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务名称：** \n",
    "\n",
    "torch.optim.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务简介：**  \n",
    "学习最常用的优化器， optim.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**详细说明：**\n",
    "\n",
    "深入了解学习率和momentum在梯度下降法中的作用，分析LR和Momentum这两个参数对优化过程的影响，最后学习optim.SGD以及pytorch的十种优化器简介。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**作业名称（详解）：** \n",
    "\n",
    "1. 优化器的作用是管理并更新参数组，请构建一个SGD优化器，通过add_param_group方法添加三组参数，三组参数的学习率分别为 0.01， 0.02， 0.03， momentum分别为0.9, 0.8, 0.7，构建好之后，并打印优化器中的param_groups属性中的每一个元素的key和value（提示：param_groups是list，其每一个元素是一个字典）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
